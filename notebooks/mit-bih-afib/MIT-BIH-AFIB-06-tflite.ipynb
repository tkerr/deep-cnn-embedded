{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdc9f81",
   "metadata": {},
   "source": [
    "# MIT-BIH-AFIB-06-tflite.ipynb\n",
    "Experiment with TensorFlow Lite using a pre-trained model and datasets from the MIT-BIH Atrial Fibrillation Database.   \n",
    "See https://physionet.org/content/afdb/1.0.0/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da79de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS name:            Windows 10\n",
      "Python version:     3.10.10\n",
      "TensorFlow version: 2.10.0\n",
      "Numpy version:      1.23.2\n",
      "You are here: D:\\dev\\jupyter\\deep-cnn-embedded\\src\\mit-bih-afib\n"
     ]
    }
   ],
   "source": [
    "# Environment setup.\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import platform\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.lite as tfl\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import fileutils as fu\n",
    "import model_utils as mu\n",
    "import mit_bih_afib_db as db\n",
    "import mit_bih_afib_tfrecord as tfr\n",
    "\n",
    "os_name = platform.system()\n",
    "print('OS name:            {} {}'.format(os_name, platform.release()))\n",
    "print('Python version:     {}.{}.{}'.format(sys.version_info[0], sys.version_info[1], sys.version_info[2]))\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('Numpy version:      {}'.format(np.__version__))\n",
    "print('You are here: {}'.format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e106613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the local data paths based on our environment.\n",
    "if (os_name == 'Windows'):\n",
    "    DATASET_PATH_ROOT = os.path.abspath(r'E:/Data/MIT-BIH-AFIB')\n",
    "    db.LOCAL_TFRECORD_PATH = os.path.join(DATASET_PATH_ROOT, 'tfrecord')\n",
    "    LOCAL_ECG_PATH = os.path.join(DATASET_PATH_ROOT, 'pkl')\n",
    "else:\n",
    "    DATASET_PATH_ROOT = os.getcwd()\n",
    "    db.LOCAL_TFRECORD_PATH = os.path.join(DATASET_PATH_ROOT, 'tfrecord')\n",
    "    LOCAL_ECG_PATH = os.path.join(DATASET_PATH_ROOT, 'pkl')\n",
    "fu.mkpath(LOCAL_ECG_PATH)  # Create the ECG data directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa13570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CSV list file: E:\\Data\\MIT-BIH-AFIB\\pkl\\test_list.csv\n"
     ]
    }
   ],
   "source": [
    "# ECG parameters.\n",
    "tfr.ECG_LENGTH = 7500\n",
    "tfr.ECG_FEATURE = 'ecg_fir_z'\n",
    "\n",
    "LABELS = {'N':0, 'AFIB':1}         # Data labels\n",
    "CLASS_NAMES = list(LABELS.keys())  # Class names in same order as labels\n",
    "NUM_CLASSES = len(LABELS)\n",
    "\n",
    "test_csv_list_file = os.path.join(LOCAL_ECG_PATH, 'test_list.csv')\n",
    "print('Test CSV list file: {}'.format(test_csv_list_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17153b20",
   "metadata": {},
   "source": [
    "### Create a test dataset from previously generated files  \n",
    "CSV data files can be loaded onto an embedded processor.  \n",
    "Also creates a master list of the data files along with their one-hot encodings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5dd729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 554\n"
     ]
    }
   ],
   "source": [
    "# Create train, test and validation lists from existing CSV files.\n",
    "# Using only the test list.\n",
    "train_file = os.path.join(db.LOCAL_TFRECORD_PATH, 'tfrecord_train_list.csv') # Needed for function but not used\n",
    "test_file = os.path.join(db.LOCAL_TFRECORD_PATH, 'tfrecord_test_list.csv')\n",
    "val_file = os.path.join(db.LOCAL_TFRECORD_PATH, 'tfrecord_val_list.csv') # Needed for function but not used\n",
    "train_list, test_list, val_list = tfr.get_tfrecord_lists(train_file, test_file, val_file, path=DATASET_PATH_ROOT)\n",
    "test_size = len(test_list)\n",
    "print('Test size: {}'.format(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5af3a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create an ECG pickle file from a TFRecord file.\n",
    "# The ECG data is converted to a Python list and written in pickle format.\n",
    "def write_ecg_pickle(tfrecord_file, ecg_tensor, local_data_path=LOCAL_ECG_PATH):\n",
    "    # Create the data file name and make a path to it.\n",
    "    ext = '.pkl'\n",
    "    basename = os.path.splitext(os.path.basename(tfrecord_file))[0]\n",
    "    pid = basename.split('_')[0]\n",
    "    basename += ext\n",
    "    filepath = os.path.join(local_data_path, pid)\n",
    "    filename = os.path.join(filepath, basename)\n",
    "    fu.mkpath(filepath)\n",
    "    \n",
    "    # Get the ECG data.\n",
    "    ecg_data = ecg_tensor.numpy().tolist()\n",
    "    \n",
    "    # Write the ECG data.\n",
    "    with open(filename, 'wb') as fd:\n",
    "        pickle.dump(ecg_data, fd)\n",
    "    \n",
    "    # Return the data file name relative to the local data path.\n",
    "    return os.path.join(pid, basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b73bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "554 files created at E:\\Data\\MIT-BIH-AFIB\\pkl.\n"
     ]
    }
   ],
   "source": [
    "# Create ECG data files.\n",
    "# Also creates a master data list CSV file.\n",
    "test_ds = tf.data.TFRecordDataset(test_list)\n",
    "test_ds = test_ds.map(tfr.ecg_map_2class)\n",
    "i = 0\n",
    "list_fd = fu.open_file(test_csv_list_file, 'w')\n",
    "\n",
    "for tds in test_ds:\n",
    "    # Get the TFRecord file name.\n",
    "    tfrecord_file = test_list[i]\n",
    "    i += 1\n",
    "    \n",
    "    # Create the ECG data file.\n",
    "    ecg_filename = write_ecg_pickle(tfrecord_file, tds[0])\n",
    "    \n",
    "    # Get the one-hot encoding.\n",
    "    one_hot = tds[1].numpy()\n",
    "           \n",
    "    # Write the file name and the one-hot encoding to the master list CSV file.\n",
    "    list_fd.write('{},{:0.1f},{:0.1f}\\n'.format(ecg_filename, one_hot[0], one_hot[1]))\n",
    "        \n",
    "fu.close_file(list_fd)\n",
    "print('{} files created at {}.'.format(i, LOCAL_ECG_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad55b0f",
   "metadata": {},
   "source": [
    "### Create TFLite models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37318c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.abspath('./checkpoint/InceptionTimeNetV4-D05-new')\n",
    "model_name = 'InceptionTimeNetV4-D05-new'\n",
    "saved_model_dir = os.path.join(model_dir, model_name)\n",
    "\n",
    "# Create a TFLite model with no optimizations (float32 parameters).\n",
    "converter = tfl.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model_f32 = converter.convert()\n",
    "file_model_f32 = os.path.join(model_dir, model_name + '-f32.tflite')\n",
    "with open(file_model_f32, 'wb') as fd:\n",
    "    fd.write(tflite_model_f32)\n",
    "\n",
    "# Create a TFLite model with optimizations (float16 parameters).\n",
    "converter.optimizations = [tfl.Optimize.DEFAULT]\n",
    "tflite_model_f16 = converter.convert()\n",
    "file_model_f16 = os.path.join(model_dir, model_name + '-f16.tflite')\n",
    "with open(file_model_f16, 'wb') as fd:\n",
    "    fd.write(tflite_model_f16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3576c7",
   "metadata": {},
   "source": [
    "### Load and test the non-optimized TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06657277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets shape: (554, 2)\n"
     ]
    }
   ],
   "source": [
    "# First, create the ground truth targets array from file list CSV file.\n",
    "targets_list = []\n",
    "with open(test_csv_list_file, 'r') as tfd:\n",
    "    for line in tfd:\n",
    "        (_, y0, y1) = line.strip().split(',')\n",
    "        targets_list.append(np.array([y0, y1], dtype=np.float32))\n",
    "targets = np.asarray(targets_list)\n",
    "print('Targets shape: {}'.format(targets.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0b45f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run predictions.\n",
    "def run_predictions(interp):\n",
    "    elapsed_time = 0.0\n",
    "    pred_list = []\n",
    "    list_fd = fu.open_file(test_csv_list_file)\n",
    "    for line in list_fd:\n",
    "        ecg_file = os.path.join(LOCAL_ECG_PATH, line.strip().split(',')[0])\n",
    "        with open(ecg_file, 'rb') as ecg_fd:\n",
    "            ecg_data = pickle.load(ecg_fd)\n",
    "        input_data = tf.constant(ecg_data, shape=(1, tfr.ECG_LENGTH, 1))\n",
    "        start = time.time()\n",
    "        interp_f32.set_tensor(input_index, input_data)\n",
    "        interp_f32.invoke()\n",
    "        output_data = interp_f32.get_tensor(output_index)\n",
    "        delta_time = time.time() - start\n",
    "        elapsed_time += delta_time\n",
    "        pred_list.append(output_data)\n",
    "    fu.close_file(list_fd)\n",
    "    print('Avg time: {:0.6f} s'.format(elapsed_time / len(pred_list)))\n",
    "    \n",
    "    # Create a classification array.\n",
    "    predictions = np.squeeze(np.asarray(pred_list), axis=1)\n",
    "    classifications = np.zeros(predictions.shape)\n",
    "    idx_array = np.argmax(predictions, axis=1)\n",
    "    for i in range(predictions.shape[0]):\n",
    "        p = predictions[i]\n",
    "        idx = idx_array[i]\n",
    "        if (p[idx] > 0.5):\n",
    "            classifications[i,idx] = 1\n",
    "            \n",
    "    return predictions, classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ca8695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute and print metrics.\n",
    "def compute_metrics(targets, classifications):\n",
    "    # Run a classification report.\n",
    "    class_report = classification_report(targets, classifications, target_names=CLASS_NAMES, zero_division=0)\n",
    "    print(class_report)\n",
    "    \n",
    "    # Additional metrics for each class.\n",
    "    for i in range(NUM_CLASSES):\n",
    "        cm = confusion_matrix(targets[:,i], classifications[:,i])\n",
    "        acc = (cm[0,0] + cm[1,1]) / np.sum(cm)\n",
    "        ppv = cm[1,1] / (cm[1,1] + cm[0,1])\n",
    "        fpr, tpr, _ = roc_curve(targets[:,i], classifications[:,i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        print('Confusion matrix for class {}:'.format(CLASS_NAMES[i]))\n",
    "        print(cm)\n",
    "        print('ACC: {:0.4f}'.format(acc))\n",
    "        print('PPV: {:0.4f}'.format(ppv))\n",
    "        print('AUC: {:0.4f}'.format(roc_auc))\n",
    "        print()\n",
    "\n",
    "    no_class_counts = np.zeros(NUM_CLASSES)\n",
    "    num_examples = targets.shape[0]\n",
    "    for i in range(num_examples):\n",
    "        if np.sum(classifications[i]) == 0:\n",
    "            no_class_counts[np.argmax(targets[i])] += 1\n",
    "    no_class_sum = np.sum(no_class_counts)\n",
    "    no_class_pct = 100. * no_class_sum / num_examples\n",
    "    print('There are {} predictions with no classification ({:0.2f}%).'.format(int(no_class_sum), no_class_pct))\n",
    "    for i in range(NUM_CLASSES):\n",
    "        print('True {:5s}: {}'.format(CLASS_NAMES[i], int(no_class_counts[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c97712df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the non-optimized model.\n",
    "interp_f32 = tfl.Interpreter(file_model_f32)\n",
    "interp_f32.allocate_tensors()\n",
    "input_index = interp_f32.get_input_details()[0]['index']\n",
    "output_index = interp_f32.get_output_details()[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e26b1cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg time: 0.153633 s\n"
     ]
    }
   ],
   "source": [
    "# Run predictions.\n",
    "predictions, classifications = run_predictions(interp_f32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "822c4b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.99      0.99      0.99       277\n",
      "        AFIB       0.99      0.99      0.99       277\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       554\n",
      "   macro avg       0.99      0.99      0.99       554\n",
      "weighted avg       0.99      0.99      0.99       554\n",
      " samples avg       0.99      0.99      0.99       554\n",
      "\n",
      "Confusion matrix for class N:\n",
      "[[274   3]\n",
      " [  2 275]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9892\n",
      "AUC: 0.9910\n",
      "\n",
      "Confusion matrix for class AFIB:\n",
      "[[275   2]\n",
      " [  3 274]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9928\n",
      "AUC: 0.9910\n",
      "\n",
      "There are 0 predictions with no classification (0.00%).\n",
      "True N    : 0\n",
      "True AFIB : 0\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics.\n",
    "compute_metrics(targets, classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d69dcd",
   "metadata": {},
   "source": [
    "### Load and test the optimized TFLite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af07b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized model.\n",
    "interp_f16 = tfl.Interpreter(file_model_f16)\n",
    "interp_f16.allocate_tensors()\n",
    "input_index = interp_f16.get_input_details()[0]['index']\n",
    "output_index = interp_f16.get_output_details()[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14d1adbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg time: 0.156243 s\n"
     ]
    }
   ],
   "source": [
    "# Run predictions.\n",
    "predictions, classifications = run_predictions(interp_f16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddf8a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.99      0.99      0.99       277\n",
      "        AFIB       0.99      0.99      0.99       277\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       554\n",
      "   macro avg       0.99      0.99      0.99       554\n",
      "weighted avg       0.99      0.99      0.99       554\n",
      " samples avg       0.99      0.99      0.99       554\n",
      "\n",
      "Confusion matrix for class N:\n",
      "[[274   3]\n",
      " [  2 275]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9892\n",
      "AUC: 0.9910\n",
      "\n",
      "Confusion matrix for class AFIB:\n",
      "[[275   2]\n",
      " [  3 274]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9928\n",
      "AUC: 0.9910\n",
      "\n",
      "There are 0 predictions with no classification (0.00%).\n",
      "True N    : 0\n",
      "True AFIB : 0\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics.\n",
    "compute_metrics(targets, classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd413d2e",
   "metadata": {},
   "source": [
    "### Check predictions run on an embedded system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b5a7ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for file: predictions-rpi-f32.pkl\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.99      0.99      0.99       277\n",
      "        AFIB       0.99      0.99      0.99       277\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       554\n",
      "   macro avg       0.99      0.99      0.99       554\n",
      "weighted avg       0.99      0.99      0.99       554\n",
      " samples avg       0.99      0.99      0.99       554\n",
      "\n",
      "Confusion matrix for class N:\n",
      "[[274   3]\n",
      " [  2 275]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9892\n",
      "AUC: 0.9910\n",
      "\n",
      "Confusion matrix for class AFIB:\n",
      "[[275   2]\n",
      " [  3 274]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9928\n",
      "AUC: 0.9910\n",
      "\n",
      "There are 0 predictions with no classification (0.00%).\n",
      "True N    : 0\n",
      "True AFIB : 0\n",
      "\n",
      "Metrics for file: predictions-rpi-f16.pkl\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.99      0.99      0.99       277\n",
      "        AFIB       0.99      0.99      0.99       277\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       554\n",
      "   macro avg       0.99      0.99      0.99       554\n",
      "weighted avg       0.99      0.99      0.99       554\n",
      " samples avg       0.99      0.99      0.99       554\n",
      "\n",
      "Confusion matrix for class N:\n",
      "[[274   3]\n",
      " [  2 275]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9892\n",
      "AUC: 0.9910\n",
      "\n",
      "Confusion matrix for class AFIB:\n",
      "[[275   2]\n",
      " [  3 274]]\n",
      "ACC: 0.9910\n",
      "PPV: 0.9928\n",
      "AUC: 0.9910\n",
      "\n",
      "There are 0 predictions with no classification (0.00%).\n",
      "True N    : 0\n",
      "True AFIB : 0\n"
     ]
    }
   ],
   "source": [
    "pred_list = ['predictions-rpi-f32.pkl', 'predictions-rpi-f16.pkl']\n",
    "for pred_file in pred_list:\n",
    "    with open(pred_file, 'rb') as pfd:\n",
    "        p_list = pickle.load(pfd)\n",
    "    predictions = np.squeeze(np.asarray(p_list), axis=1)\n",
    "    \n",
    "    # Create a classifications array.\n",
    "    classifications = np.zeros(predictions.shape)\n",
    "    idx_array = np.argmax(predictions, axis=1)\n",
    "    for i in range(predictions.shape[0]):\n",
    "        p = predictions[i]\n",
    "        idx = idx_array[i]\n",
    "        if (p[idx] > 0.5):\n",
    "            classifications[i,idx] = 1\n",
    "    \n",
    "    # Compute metrics.\n",
    "    print('\\nMetrics for file: {}'.format(pred_file))\n",
    "    compute_metrics(targets, classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d7e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dabbed0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
